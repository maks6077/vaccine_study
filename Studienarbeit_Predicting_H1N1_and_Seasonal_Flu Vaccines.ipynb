{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13ffa6f",
   "metadata": {},
   "source": [
    "# Cleaning the training-set and dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load data\n",
    "file_path = \"training_set_features.csv\"\n",
    "labels_file_path = \"training_set_labels.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "labels = pd.read_csv(labels_file_path)\n",
    "\n",
    "# calculation of missing values per column\n",
    "missing_values_abs = data.isnull().sum()\n",
    "missing_values_percent = (data.isnull().sum() / data.shape[0]) * 100\n",
    "\n",
    "# summarize missing values in a DataFrame\n",
    "missing_values_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values_abs,\n",
    "    'Percentage (%)': missing_values_percent\n",
    "}).sort_values(by='Missing Values', ascending=False)\n",
    "print(missing_values_summary)\n",
    "\n",
    "\n",
    "# visualization of missing values\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('missing values in dataset')\n",
    "plt.show()\n",
    "\n",
    "# save missing values in CSV\n",
    "missing_values_summary_path = 'missing_values_summary_trainingset.csv'\n",
    "missing_values_summary.to_csv(missing_values_summary_path)\n",
    "\n",
    "# delete records with 19 or more missing values\n",
    "data['missing_values_count'] = data.isnull().sum(axis=1)\n",
    "ids_to_remove = data[data['missing_values_count'] >= 19]['respondent_id']\n",
    "print(\"IDs entfernt:\", ids_to_remove.tolist())\n",
    "data_cleaned = data[~data['respondent_id'].isin(ids_to_remove)].copy()\n",
    "labels_cleaned = labels[~labels['respondent_id'].isin(ids_to_remove)].copy()\n",
    "\n",
    "\n",
    "# analysis of distribution of missing values\n",
    "missing_values_distribution = data['missing_values_count'].value_counts().sort_index()\n",
    "print(missing_values_distribution)\n",
    "data_cleaned.drop(['missing_values_count'], axis=1, inplace=True)\n",
    "\n",
    "# replace with \"missing\" if no value in this column\n",
    "for col in ['employment_occupation', 'employment_industry', 'health_insurance', \"opinion_h1n1_vacc_effective\", 'income_poverty', 'rent_or_own', 'employment_status',\n",
    "             \"opinion_h1n1_risk\", \"opinion_h1n1_sick_from_vacc\", \"opinion_seas_vacc_effective\", \"opinion_seas_risk\", \"opinion_seas_sick_from_vacc\", 'education']:\n",
    "    data_cleaned[col].fillna('missing', inplace=True)\n",
    "\n",
    "# imputation for numerical variables with median\n",
    "numerical_cols = ['household_adults', 'household_children']\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "data_cleaned[numerical_cols] = num_imputer.fit_transform(data_cleaned[numerical_cols])\n",
    "\n",
    "# imputation for the remaining variables with most frequently occurring value\n",
    "categorical_cols = ['marital_status', 'doctor_recc_h1n1', \n",
    "                    'doctor_recc_seasonal', 'chronic_med_condition', \"behavioral_avoidance\", \"behavioral_face_mask\", \"behavioral_wash_hands\",\n",
    "                    'child_under_6_months', 'health_worker', 'h1n1_concern', 'h1n1_knowledge', 'behavioral_antiviral_meds', \"behavioral_large_gatherings\",\n",
    "                    \"behavioral_outside_home\", \"behavioral_touch_face\"]\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data_cleaned[categorical_cols] = cat_imputer.fit_transform(data_cleaned[categorical_cols])\n",
    "\n",
    "\n",
    "# save cleaned data in CSV\n",
    "data_cleaned.to_csv('training_set_features_cleaned.csv', index=False)\n",
    "labels_cleaned.to_csv('training_set_labels_cleaned.csv', index=False)\n",
    "print(\"Preprocessing abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7cbe1e",
   "metadata": {},
   "source": [
    "# Cleaning the test-set and dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load data\n",
    "file_path = \"test_set_features.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# calculation of missing values per column\n",
    "missing_values_abs = data.isnull().sum()\n",
    "missing_values_percent = (data.isnull().sum() / data.shape[0]) * 100\n",
    "\n",
    "# summarize missing values in a DataFrame\n",
    "missing_values_summary = pd.DataFrame({\n",
    "    'Missing Values': missing_values_abs,\n",
    "    'Percentage (%)': missing_values_percent\n",
    "}).sort_values(by='Missing Values', ascending=False)\n",
    "print(missing_values_summary)\n",
    "\n",
    "\n",
    "# visualization of missing values\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title('missing values in dataset')\n",
    "plt.show()\n",
    "\n",
    "# save missing values in CSV\n",
    "missing_values_summary_path = 'missing_values_summary_testset.csv'\n",
    "missing_values_summary.to_csv(missing_values_summary_path)\n",
    "\n",
    "\n",
    "# replace with \"missing\" if no value in this column\n",
    "for col in ['employment_occupation', 'employment_industry', 'health_insurance', \"opinion_h1n1_vacc_effective\", 'income_poverty', 'rent_or_own', 'employment_status',\n",
    "             \"opinion_h1n1_risk\", \"opinion_h1n1_sick_from_vacc\", \"opinion_seas_vacc_effective\", \"opinion_seas_risk\", \"opinion_seas_sick_from_vacc\", 'education']:\n",
    "    data[col].fillna('missing', inplace=True)\n",
    "\n",
    "# imputation for numerical variables by the median\n",
    "numerical_cols = ['household_adults', 'household_children']\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "data[numerical_cols] = num_imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "# imputation for the remaining variables with the most frequently occurring value\n",
    "categorical_cols = ['marital_status', 'doctor_recc_h1n1', \n",
    "                    'doctor_recc_seasonal', 'chronic_med_condition', \"behavioral_avoidance\", \"behavioral_face_mask\", \"behavioral_wash_hands\",\n",
    "                    'child_under_6_months', 'health_worker', 'h1n1_concern', 'h1n1_knowledge', 'behavioral_antiviral_meds', \"behavioral_large_gatherings\",\n",
    "                    \"behavioral_outside_home\", \"behavioral_touch_face\"]\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[categorical_cols] = cat_imputer.fit_transform(data[categorical_cols])\n",
    "\n",
    "\n",
    "# save cleaned data in CSV\n",
    "data.to_csv('test_set_features_cleaned.csv', index=False)\n",
    "print(\"Preprocessing abgeschlossen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a7b8d",
   "metadata": {},
   "source": [
    "# Examining the data and plotting demographical characterictics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c60fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# load data\n",
    "features_path = 'training_set_features.csv'\n",
    "features = pd.read_csv(features_path)\n",
    "\n",
    "# set age group order for plot\n",
    "age_order = ['65+ Years', '55 - 64 Years', '45 - 54 Years', '35 - 44 Years','18 - 34 Years']\n",
    "\n",
    "# making sure categorical data is formatted correctly\n",
    "features['age_group'] = pd.Categorical(features['age_group'], categories=age_order, ordered=True)\n",
    "features['education'] = features['education'].astype('category')\n",
    "features['sex'] = features['sex'].astype('category')\n",
    "features['race'] = features['race'].astype('category')\n",
    "\n",
    "\n",
    "### Plots\n",
    "\n",
    "# age group\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(4, 1, 1)\n",
    "sns.countplot(y='age_group', data=features, order=age_order)\n",
    "plt.title('Distribution by Age Group')\n",
    "plt.xlabel('Number of Respondents')\n",
    "\n",
    "# sex\n",
    "plt.subplot(4, 1, 2)\n",
    "sns.countplot(y='sex', data=features)\n",
    "plt.title('Distribution by Gender')\n",
    "plt.xlabel('Number of Respondents')\n",
    "\n",
    "# education\n",
    "plt.subplot(4, 1, 3)\n",
    "sns.countplot(y='education', data=features, order=features['education'].value_counts().index)\n",
    "plt.title('Distribution by Education Level')\n",
    "plt.xlabel('Number of Respondents')\n",
    "\n",
    "# race\n",
    "plt.subplot(4, 1, 4)\n",
    "sns.countplot(y='race', data=features, order=features['race'].value_counts().index)\n",
    "plt.title('Distribution by Race')\n",
    "plt.xlabel('Number of Respondents')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78033afb",
   "metadata": {},
   "source": [
    "# Data understanding: plotting distribution of target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "import seaborn as sns\n",
    "\n",
    "# load data\n",
    "labels_path = 'training_set_labels_cleaned.csv'\n",
    "features_path = 'training_set_features_cleaned.csv'\n",
    "labels = pd.read_csv(labels_path)\n",
    "features = pd.read_csv(features_path)\n",
    "\n",
    "\n",
    "# configure plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "colors = [\"#E74C3C\", \"#2ECC71\"] \n",
    "\n",
    "# plot H1N1-vaccine distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "ax1 = sns.countplot(x='h1n1_vaccine', data=labels, palette=colors)\n",
    "plt.title('distribution H1N1 vaccine')\n",
    "total_h1n1 = len(labels['h1n1_vaccine'])\n",
    "for p in ax1.patches:\n",
    "    height = p.get_height()\n",
    "    ax1.text(p.get_x()+p.get_width()/2., height, '{:1.2f}%'.format(100 * height/total_h1n1), ha=\"center\")\n",
    "\n",
    "# plot seasonal vaccine distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "ax2 = sns.countplot(x='seasonal_vaccine', data=labels, palette=colors)\n",
    "plt.title('distribution seasonal vaccine')\n",
    "total_seasonal = len(labels['seasonal_vaccine'])\n",
    "for p in ax2.patches:\n",
    "    height = p.get_height()\n",
    "    ax2.text(p.get_x()+p.get_width()/2., height, '{:1.2f}%'.format(100 * height/total_seasonal), ha=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab10065",
   "metadata": {},
   "source": [
    "# Data understanding: Chi-square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472acb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# load data\n",
    "features_path = 'training_set_features_cleaned.csv'\n",
    "labels_path = 'training_set_labels_cleaned.csv'\n",
    "features = pd.read_csv(features_path)\n",
    "labels = pd.read_csv(labels_path)\n",
    "\n",
    "# merge features and labels\n",
    "data_combined = pd.merge(features, labels, on='respondent_id')\n",
    "\n",
    "# list of categorical columns\n",
    "categorical_cols = ['employment_occupation', 'employment_industry',\n",
    "                    \"opinion_h1n1_sick_from_vacc\", \"opinion_seas_vacc_effective\",\n",
    "                    'income_poverty','education', \n",
    "                    'doctor_recc_seasonal', 'chronic_med_condition', \n",
    "                    'child_under_6_months', 'health_worker', \n",
    "                    ]  \n",
    "\n",
    "# store Chi-square test results\n",
    "chi2_results = []\n",
    "\n",
    "for var in categorical_cols:\n",
    "    contingency_table = pd.crosstab(data_combined[var], data_combined['h1n1_vaccine'])\n",
    "    chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
    "    chi2_results.append((var, p))\n",
    "\n",
    "\n",
    "chi2_results.sort(key=lambda x: x[1])\n",
    "\n",
    "\n",
    "# extract variables and p-values \n",
    "variables, p_values = zip(*chi2_results)\n",
    "\n",
    "# plot p-values\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(variables, [-np.log10(p) for p in p_values])\n",
    "plt.xlabel('-log10(p-value)', fontsize=18)\n",
    "plt.ylabel('Variables',fontsize=25)\n",
    "plt.title('p-values of Chi-squared test for categorical variables', fontsize=22)\n",
    "plt.tick_params(axis='y', which='major', labelsize=14)\n",
    "plt.tick_params(axis='x', which='major', labelsize=12)\n",
    "plt.gca().invert_yaxis() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5bc302",
   "metadata": {},
   "source": [
    "# One-Hot Encoding of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "file_path = \"training_set_features_cleaned.csv\"\n",
    "file_path2 = \"test_set_features_cleaned.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "test_data = pd.read_csv(file_path2)\n",
    "\n",
    "# list of categorical columns\n",
    "categorical_cols = ['employment_occupation', 'employment_industry', 'health_insurance', \"opinion_h1n1_vacc_effective\",\n",
    "                   \"opinion_h1n1_risk\", \"opinion_h1n1_sick_from_vacc\", \"opinion_seas_vacc_effective\", \"opinion_seas_risk\", \"opinion_seas_sick_from_vacc\",\n",
    "                    'income_poverty', 'rent_or_own', 'employment_status', \n",
    "                    'marital_status', 'education', 'doctor_recc_h1n1', \n",
    "                    'doctor_recc_seasonal', 'chronic_med_condition', \n",
    "                    'child_under_6_months', 'health_worker', 'age_group', 'race', 'sex', \n",
    "                    'hhs_geo_region', 'census_msa']  \n",
    "\n",
    "# one-hot-encoding\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "test_data_encoded = pd.get_dummies(test_data, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "# save encoded data\n",
    "data_encoded.to_csv(\"training_set_features_encoded.csv\", index=False)\n",
    "test_data_encoded.to_csv(\"test_set_features_encoded.csv\", index=False)\n",
    "\n",
    "print(\"One-Hot-Encoding abgeschlossen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7107c788",
   "metadata": {},
   "source": [
    "# Making sure data got cleaned and transformed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf443e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "file_path = \"training_set_features_cleaned.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# check for respondent_id's with 20 or more missing values\n",
    "data['missing_values_count'] = data.isnull().sum(axis=1)\n",
    "ids_to_remove = data[data['missing_values_count'] >= 20]['respondent_id']\n",
    "print(\"IDs zum Entfernen:\", ids_to_remove.tolist())\n",
    "\n",
    "# list of categorical columns\n",
    "categorical_cols = ['employment_occupation', 'employment_industry', 'health_insurance', \"opinion_h1n1_vacc_effective\",\n",
    "                   \"opinion_h1n1_risk\", \"opinion_h1n1_sick_from_vacc\", \"opinion_seas_vacc_effective\", \"opinion_seas_risk\", \"opinion_seas_sick_from_vacc\",\n",
    "                    'income_poverty', 'rent_or_own', 'employment_status', \n",
    "                    'marital_status', 'education', 'doctor_recc_h1n1', \n",
    "                    'doctor_recc_seasonal', 'chronic_med_condition', \n",
    "                    'child_under_6_months', 'health_worker']  \n",
    "\n",
    "# check for unique values\n",
    "for col in categorical_cols:\n",
    "    print(f\"Einzigartige Werte in {col}: {data[col].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d653e9ba",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for kNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25619fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "features_path = 'training_set_features_encoded.csv'\n",
    "labels_path = 'training_set_labels.csv'\n",
    "features = pd.read_csv(features_path)\n",
    "labels = pd.read_csv(labels_path)\n",
    "\n",
    "# merge features and labels\n",
    "data = features.merge(labels, on='respondent_id')\n",
    "\n",
    "# split data into features (X) and labels (y) for both models\n",
    "X = data.drop(['respondent_id', 'h1n1_vaccine', 'seasonal_vaccine'], axis=1)\n",
    "y_h1n1 = data['h1n1_vaccine']\n",
    "y_seasonal = data['seasonal_vaccine']\n",
    "\n",
    "# split into training and test set\n",
    "X_train, X_test, y_train_h1n1, y_test_h1n1 = train_test_split(X, y_h1n1, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_seasonal, y_test_seasonal = train_test_split(X, y_seasonal, test_size=0.2, random_state=42)\n",
    "\n",
    "# define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [2, 5, 10, 30, 75, 106, 160],\n",
    "    'metric': ['euclidean', 'manhattan', 'hamming']\n",
    "}\n",
    "\n",
    "# function to show top 10 results\n",
    "def display_top_10_results(grid_search):\n",
    "    cv_results = grid_search.cv_results_\n",
    "    top10_idx = np.argsort(cv_results['mean_test_score'])[-10:]\n",
    "    print(\"Top 10 Scores und ihre Parameter:\")\n",
    "    for idx in top10_idx[::-1]:\n",
    "        print(f\"Score: {cv_results['mean_test_score'][idx]:.4f}, Params: {cv_results['params'][idx]}\")\n",
    "\n",
    "# GridSearchCV for h1n1 model\n",
    "grid_search_h1n1 = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', verbose=3)\n",
    "grid_search_h1n1.fit(X_train, y_train_h1n1)\n",
    "print(\"Beste Parameter für H1N1-Modell:\", grid_search_h1n1.best_params_)\n",
    "display_top_10_results(grid_search_h1n1)\n",
    "\n",
    "# GridSearchCV for seasonal model\n",
    "grid_search_seasonal = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy', verbose=3)\n",
    "grid_search_seasonal.fit(X_train, y_train_seasonal)\n",
    "print(\"Beste Parameter für saisonales Modell:\", grid_search_seasonal.best_params_)\n",
    "display_top_10_results(grid_search_seasonal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c8d500",
   "metadata": {},
   "source": [
    "# kNN model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66bed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load data\n",
    "features_path = 'training_set_features_encoded.csv'\n",
    "labels_path = 'training_set_labels.csv'\n",
    "test_features_path = 'test_set_features_encoded.csv'\n",
    "features = pd.read_csv(features_path)\n",
    "labels = pd.read_csv(labels_path)\n",
    "test_features = pd.read_csv(test_features_path)\n",
    "\n",
    "# merge features and labels\n",
    "data = features.merge(labels, on='respondent_id')\n",
    "\n",
    "# split data into features (X) and labels (y) for both models\n",
    "X = data.drop(['respondent_id', 'h1n1_vaccine', 'seasonal_vaccine'], axis=1)\n",
    "y_h1n1 = data['h1n1_vaccine']\n",
    "y_seasonal = data['seasonal_vaccine']\n",
    "\n",
    "# drop respondent_id from testset\n",
    "X_test = test_features.drop(['respondent_id'], axis=1)\n",
    "\n",
    "# model training for h1n1\n",
    "knn_h1n1 = KNeighborsClassifier(metric='manhattan', n_neighbors=75)\n",
    "knn_h1n1.fit(X, y_h1n1)\n",
    "\n",
    "# model training for seasonal\n",
    "knn_seasonal = KNeighborsClassifier(metric='hamming', n_neighbors=75)\n",
    "knn_seasonal.fit(X, y_seasonal)\n",
    "\n",
    "# make predictions for testset\n",
    "h1n1_vaccine_pred = knn_h1n1.predict_proba(X_test)[:, 1]\n",
    "seasonal_vaccine_pred = knn_seasonal.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# saving predictions in CSV\n",
    "submission = test_features[['respondent_id']].copy()\n",
    "submission['h1n1_vaccine'] = h1n1_vaccine_pred\n",
    "submission['seasonal_vaccine'] = seasonal_vaccine_pred\n",
    "submission.to_csv('submission_knn.csv', index=False)\n",
    "\n",
    "print(\"kNN Predictions erfolgreich gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca6498",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e8052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# load data\n",
    "features_path = 'training_set_features_encoded.csv'\n",
    "labels_path = 'training_set_labels_cleaned.csv'\n",
    "features = pd.read_csv(features_path)\n",
    "labels = pd.read_csv(labels_path)\n",
    "X = features.drop(['respondent_id'], axis=1)\n",
    "y_h1n1 = labels['h1n1_vaccine']\n",
    "y_seasonal = labels['seasonal_vaccine']\n",
    "\n",
    "# hyperparameter tuning\n",
    "def objective(X, y, trial):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    C = trial.suggest_loguniform('C', 1e-3, 1e3)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    \n",
    "    # SVM model\n",
    "    clf = SVC(C=C, kernel=kernel, gamma=gamma, probability=True)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    preds = clf.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, preds)\n",
    "    return accuracy\n",
    "\n",
    "# optuna study for H1N1\n",
    "study_h1n1 = optuna.create_study(direction='maximize', sampler=TPESampler())\n",
    "study_h1n1.optimize(partial(objective, X, y_h1n1), n_trials=100)\n",
    "\n",
    "print(\"H1N1 beste Parameter:\", study_h1n1.best_params)\n",
    "print(\"H1N1 bester Cross-Validation Score:\", study_h1n1.best_value)\n",
    "\n",
    "# optuna study for seasonal\n",
    "study_seasonal = optuna.create_study(direction='maximize', sampler=TPESampler())\n",
    "study_seasonal.optimize(partial(objective, X, y_seasonal), n_trials=100)\n",
    "\n",
    "print(\"\\nseasonal beste Parameter:\", study_seasonal.best_params)\n",
    "print(\"seasonal bester Cross-Validation Score:\", study_seasonal.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff58a9e",
   "metadata": {},
   "source": [
    "# SVM model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bb837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# load data\n",
    "features_path = 'training_set_features_encoded.csv'\n",
    "labels_path = 'training_set_labels_cleaned.csv'\n",
    "test_features_path = 'test_set_features_encoded.csv'\n",
    "submission_format_path = 'submission_format.csv'\n",
    "features = pd.read_csv(features_path)\n",
    "labels = pd.read_csv(labels_path)\n",
    "test_features = pd.read_csv(test_features_path)\n",
    "submission_format = pd.read_csv(submission_format_path)\n",
    "\n",
    "# preparing data\n",
    "X = features.drop(['respondent_id'], axis=1)\n",
    "y_h1n1 = labels['h1n1_vaccine']\n",
    "y_seasonal = labels['seasonal_vaccine']\n",
    "X_test_submission = test_features.drop(['respondent_id'], axis=1)\n",
    "\n",
    "# scaling of features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_submission_scaled = scaler.transform(X_test_submission)\n",
    "\n",
    "# defining hyperparameters\n",
    "optimal_params = {'C': 0.022770434858826624, 'kernel': 'linear', 'gamma': 'auto'}\n",
    "\n",
    "# initialise models for H1N1 and seasonal\n",
    "svm_h1n1 = SVC(**optimal_params, probability=True)\n",
    "svm_seasonal = SVC(**optimal_params, probability=True)\n",
    "\n",
    "# train SVM model for H1N1\n",
    "svm_h1n1.fit(X_scaled, y_h1n1)\n",
    "\n",
    "# train SVM model for seasonal\n",
    "svm_seasonal.fit(X_scaled, y_seasonal)\n",
    "\n",
    "# make predictions for testset\n",
    "h1n1_proba = svm_h1n1.predict_proba(X_test_submission_scaled)[:, 1]\n",
    "seasonal_proba = svm_seasonal.predict_proba(X_test_submission_scaled)[:, 1]\n",
    "\n",
    "# save submission file\n",
    "submission = submission_format.copy()\n",
    "submission['h1n1_vaccine'] = h1n1_proba\n",
    "submission['seasonal_vaccine'] = seasonal_proba\n",
    "submission.to_csv('submission_svm.csv', index=False)\n",
    "\n",
    "print(\"SVM submission erfolgreich gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050ee5c2",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for XGB model (H1N1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98902610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load data\n",
    "features_path = 'training_set_features_encoded.csv'\n",
    "labels_path = 'training_set_labels.csv'\n",
    "features = pd.read_csv(features_path)\n",
    "labels = pd.read_csv(labels_path)\n",
    "\n",
    "# merge features and labels and clean special characters in columns\n",
    "data = features.merge(labels, on='respondent_id')\n",
    "data.columns = [col.replace(\">\", \"gt\").replace(\"<\", \"lt\").replace(\"[\", \"\").replace(\"]\", \"\") for col in data.columns]\n",
    "\n",
    "# split data into features (X) and labels (y) for H1N1\n",
    "X = data.drop(['respondent_id', 'h1n1_vaccine', 'seasonal_vaccine'], axis=1)\n",
    "y_h1n1 = data['h1n1_vaccine']\n",
    "\n",
    "# split into training and test set\n",
    "X_train, X_test, y_train_h1n1, y_test_h1n1 = train_test_split(X, y_h1n1, test_size=0.2, random_state=42)\n",
    "\n",
    "# hyperparameter tuning\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'metric': 'logloss',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    clf = xgb.XGBClassifier(**param)\n",
    "    clf.fit(X_train, y_train_h1n1, eval_set=[(X_test, y_test_h1n1)], early_stopping_rounds=50, verbose=False)\n",
    "    preds = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test_h1n1, preds)\n",
    "    return accuracy\n",
    "\n",
    "# create optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "\n",
    "# show top 10 studies (parameter combinations)\n",
    "top10_trials = sorted(study.trials, key=lambda trial: trial.value, reverse=True)[:10]\n",
    "for i, trial in enumerate(top10_trials, start=1):\n",
    "    print(f\"Rank {i}: Score {trial.value}, Params {trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2f3ee",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for XGB model (seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load data\n",
    "features_path = 'training_set_features_encoded.csv'\n",
    "labels_path = 'training_set_labels.csv'\n",
    "features = pd.read_csv(features_path)\n",
    "labels = pd.read_csv(labels_path)\n",
    "\n",
    "# merge features and labels and clean special characters in columns\n",
    "data = features.merge(labels, on='respondent_id')\n",
    "data.columns = [col.replace(\">\", \"gt\").replace(\"<\", \"lt\").replace(\"[\", \"\").replace(\"]\", \"\") for col in data.columns]\n",
    "\n",
    "# split data into features (X) and labels (y) for seasonal\n",
    "X = data.drop(['respondent_id', 'h1n1_vaccine', 'seasonal_vaccine'], axis=1)\n",
    "y_seasonal = data['seasonal_vaccine']\n",
    "\n",
    "# split into training and test set\n",
    "X_train, X_test, y_train_seasonal, y_test_seasonal = train_test_split(X, y_seasonal, test_size=0.2, random_state=42)\n",
    "\n",
    "# hyperparameter tuning\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'metric': 'logloss',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "    }\n",
    "\n",
    "    clf = xgb.XGBClassifier(**param)\n",
    "    clf.fit(X_train, y_train_seasonal, eval_set=[(X_test, y_test_seasonal)], early_stopping_rounds=50, verbose=False)\n",
    "    preds = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test_seasonal, preds)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "\n",
    "# show top 10 studies (parameter combinations)\n",
    "top10_trials = sorted(study.trials, key=lambda trial: trial.value, reverse=True)[:10]\n",
    "for i, trial in enumerate(top10_trials, start=1):\n",
    "    print(f\"Rank {i}: Score {trial.value}, Params {trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392f640",
   "metadata": {},
   "source": [
    "# XGB model implementation (and plotting feature importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load data\n",
    "features_path = 'training_set_features_encoded.csv'\n",
    "labels_path = 'training_set_labels_cleaned.csv'\n",
    "test_features_path = 'test_set_features_encoded.csv'\n",
    "submission_format_path = 'submission_format.csv'\n",
    "features = pd.read_csv(features_path)\n",
    "labels = pd.read_csv(labels_path)\n",
    "test_features = pd.read_csv(test_features_path)\n",
    "submission_format = pd.read_csv(submission_format_path)\n",
    "\n",
    "\n",
    "# clean column names as model has problems with those symbols\n",
    "def clean_column_names(df):\n",
    "    df.columns = [col.replace(\">\", \"gt\").replace(\"<\", \"lt\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"=\", \"eq\") for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "# data preperation\n",
    "X = features.drop(['respondent_id'], axis=1)\n",
    "y_h1n1 = labels['h1n1_vaccine']\n",
    "y_seasonal = labels['seasonal_vaccine']\n",
    "X_test_submission = test_features.drop(['respondent_id'], axis=1)\n",
    "X = clean_column_names(X)\n",
    "X_test_submission = clean_column_names(X_test_submission)\n",
    "\n",
    "# hyper parameters\n",
    "params_h1n1 = {'n_estimators': 837, 'learning_rate': 0.059377378770111296, 'max_depth': 3, 'subsample': 0.9448834828833715, 'colsample_bytree': 0.8180301609939452}\n",
    "params_seasonal = {'n_estimators': 429, 'learning_rate': 0.02729221792750674, 'max_depth': 9, 'subsample': 0.8743368970026256, 'colsample_bytree': 0.698013566864418}\n",
    "\n",
    "# initialize and train models\n",
    "model_h1n1 = xgb.XGBClassifier(**params_h1n1, use_label_encoder=False, eval_metric='logloss')\n",
    "model_seasonal = xgb.XGBClassifier(**params_seasonal, use_label_encoder=False, eval_metric='logloss')\n",
    "model_h1n1.fit(X, y_h1n1)\n",
    "model_seasonal.fit(X, y_seasonal)\n",
    "\n",
    "# predictions for the testset\n",
    "h1n1_proba = model_h1n1.predict_proba(X_test_submission)[:, 1]\n",
    "seasonal_proba = model_seasonal.predict_proba(X_test_submission)[:, 1]\n",
    "\n",
    "\n",
    "### plot feature importance for both models\n",
    "fig, axs = plt.subplots(2, 1, figsize=(20, 6))\n",
    "\n",
    "# H1N1\n",
    "xgb.plot_importance(model_h1n1, max_num_features=10, importance_type='weight', height=0.5, ax=axs[0], show_values=False)\n",
    "axs[0].set_title('XGBoost: Feature Importance for H1N1 Vaccine')\n",
    "axs[0].grid(False) \n",
    "# seasonal\n",
    "xgb.plot_importance(model_seasonal, max_num_features=10, importance_type='weight', height=0.5, ax=axs[1], show_values=False)\n",
    "axs[1].set_title('XGBoost: Feature Importance for Seasonal Flu Vaccine')\n",
    "axs[1].grid(False) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# create submission file\n",
    "submission = submission_format.copy()\n",
    "submission['h1n1_vaccine'] = h1n1_proba\n",
    "submission['seasonal_vaccine'] = seasonal_proba\n",
    "\n",
    "submission.to_csv('submission_xgb.csv', index=False)\n",
    "\n",
    "print(\"submission saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
